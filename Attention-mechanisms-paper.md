# 注意力机制在自然语言处理方面的文章笔记

- 个人笔记，记载极其不全面，欢迎补充
- 2018年以后文章
- AAAI/IJCAI/ACL/EMNLP等会议


# [2018]

1. [Hierarchical Attention Flow for Multiple-Choice Reading Comprehension](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16331/16177) Haichao Zhu, Furu Wei, Bing Qin, Ting Liu  AAAI, 2018.

2. [Dual Attention Network for Product Compatibility and Function Satisfiability Analysis](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/17294/16169) Hu Xu，Sihong Xie，Lei Shu，Philip S. Yu，AAAI，2018.

3. [Improving Neural Fine-Grained Entity Typing with Knowledge Attention](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16321/16167)Ji Xin, Yankai Lin, Zhiyuan Liu, Maosong Sun，AAAI，2018.

4. [Improving Review Representations with User Attention and Product Attention for Sentiment Classification](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16728/16166)Zhen Wu, Xin-Yu Dai, ∗ Cunyan Yin, Shujian Huang, Jiajun Chen，AAAI，2018.

5. [Mention and Entity Description Co-Attention for Entity Disambiguation](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16382/16156)Feng Nie, Yunbo Cao, Jinpeng Wang, Chin-Yew Lin, Rong Pan，AAAI，2018.

6. [Hierarchical Attention Transfer Network for Cross-Domain Sentiment Classification](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16873/16149) Zheng Li, Ying Wei, Yu Zhang, Qiang Yang，AAAI，2018.

7. [A Question-Focused Multi-Factor Attention Network for Question Answering](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/17226/16146) Souvik Kundu, Hwee Tou Ng，AAAI，2018.

8. [RNN-Based Sequence-Preserved Attention for Dependency Parsing](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/17176/16135) Yi Zhou, Junying Zhou, Lu Liu, Jiangtao Feng, Haoyuan Peng, Xiaoqing Zheng，AAAI，2018.

9. [Adaptive Co-Attention Network for Named Entity Recognition in Tweets](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16432/16127) Qi Zhang, Jinlan Fu, Xiaoyu Liu, Xuanjing Huang，AAAI，2018.

10. [Chinese LIWC Lexicon Expansion via Hierarchical Classification of Word Embeddings with Sememe Attention](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16760/16124)Xiangkai Zeng, Cheng Yang, Cunchao Tu, Zhiyuan Liu, Maosong Sun，AAAI，2018.

11. [Multi-Attention Recurrent Network for Human Communication Comprehension](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/17390/16123) Amir Zadeh，Paul Pu Liang，Soujanya Poria，Prateek Vij, Erik Cambria, Louis-Philippe Morency, AAAI，2018.

12. [Hierarchical Recurrent Attention Network for Response Generation](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16510/16119) Chen Xing, Yu Wu, Wei Wu, Yalou Huang, Ming Zhou，AAAI，2018.

13. [Word Attention for Sequence to Sequence Text Understanding](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16598/16115) Lijun Wu, Fei Tian, Li Zhao, Jianhuang Lai, Tie-Yan Liu，AAAI，2018.

14. [DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16126/16099) Tao Shen, Jing Jiang, Tianyi Zhou, Shirui Pan, Guodong Long, Chengqi Zhang，AAAI，2018.

15. [Attention-Based Belief or Disbelief Feature Extraction for Dependency Parsing](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/17121/16091)Haoyuan Peng, Lu Liu, Yi Zhou, Junying Zhou, Xiaoqing Zheng，AAAI，2018.

16. [An Unsupervised Model with Attention Autoencoders for Question Retrieval](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16662/16031) Minghua Zhang, Yunfang Wu，AAAI，2018.

17. [Deep Semantic Role Labeling with Self-Attention](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16725/16025) Zhixing Tan, Mingxuan Wang, Jun Xie, Yidong Chen, Xiaodong Shi ，AAAI，2018.

18. [Event Detection via Gated Multilingual Attention Mechanism](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16371/16017)Jian Liu, Yubo Chen, Kang Liu, Jun Zhao，AAAI，2018.

19. [Neural Knowledge Acquisition via Mutual Attention between Knowledge Graph and Text](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16691/16013)Xu Han, Zhiyuan Liu, Maosong Sun，AAAI，2018.

20. [Syntax-Directed Attention for Neural Machine Translation](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16060/16008) Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita, Tiejun Zhao，AAAI，2018.

21. [Attend and Diagnose: Clinical Time Series Analysis Using Attention Models](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16325/16790) Huan Song, † Deepta Rajan, ‡∗ Jayaraman J. Thiagarajan, †† Andreas Spanias，AAAI，2018.

22. [Attention-Based Transactional Context Embedding for Next-Item Recommendation](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16318/15972) † Shoujin Wang, † Liang Hu, † Longbing Cao, ‡ Xiaoshui Huang, * Defu Lian, † Wei Liu，AAAI，2018.

23. [Attention-via-Attention Neural Machine Translation](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16534/15733)Shenjian Zhao，Zhihua Zhang，AAAI，2018.

24. [Visual Attention Model for Name Tagging in Multimodal Social Media](http://aclweb.org/anthology/P18-1185)Di Lu, Leonardo Neves, Vitor Carvalho, Ning Zhang, Heng Ji，ACL，2018.

25. [Attention Focusing for Neural Machine Translation by Bridging Source and Target Embeddings](http://aclweb.org/anthology/P18-1164)Shaohui Kuang, Junhui Li, António Branco, Weihua Luo, Deyi Xiong，ACL，2018.

26. [ Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network](http://aclweb.org/anthology/P18-1103)Xiangyang Zhou, Lu Li, Daxiang Dong, Yi Liu, Ying Chen, Wayne Xin Zhao, Dianhai Yu, Hua Wu，ACL，2018.

27. [Cold-Start Aware User and Product Attention for Sentiment Classification](http://aclweb.org/anthology/P18-1236)Reinald Kim Amplayo, Jihyeok Kim, Sua Sung, Seung-won Hwang，ACL，2018.

28. [Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering](http://aclweb.org/anthology/P18-1158)Wei Wang, Ming Yan, Chen Wu，ACL，2018.


29. [Multi-Input Attention for Unsupervised OCR Correction](http://aclweb.org/anthology/P18-1220)Rui Dong, David Smith，ACL，2018.

30. [ How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures](http://aclweb.org/anthology/P18-1167)Tobias Domhan，ACL，2018.

31. [Accelerating Neural Transformer via an Average Attention Network](http://aclweb.org/anthology/P18-1166)Biao Zhang, Deyi Xiong, jinsong su，ACL，2018.

32. [ Multimodal Affective Analysis Using Hierarchical Attention Strategy with Word-Level Alignment](http://aclweb.org/anthology/P18-1207)Yue Gu, Kangning Yang, Shiyu Fu, Shuhong Chen, Xinyu Li, Ivan Marsic，ACL，2018.

33. [ Document Modeling with External Attention for Sentence Extraction](http://aclweb.org/anthology/P18-1188)Shashi Narayan, Ronald Cardenas, Nikos Papasarantopoulos, Shay B. Cohen, Mirella Lapata, Jiangsheng Yu, Yi Chang，ACL，2018.

34. [ Efficient Large-Scale Neural Domain Classification with Personalized Attention](http://aclweb.org/anthology/P18-1206)Young-Bum Kim, Dongchan Kim, Anjishnu Kumar, Ruhi Sarikaya，ACL，2018.

35. [ Neural Coreference Resolution with Deep Biaffine Attention by Joint Mention Detection and Mention Clustering](http://aclweb.org/anthology/P18-2017)Rui Zhang, Cicero Nogueira dos Santos, Michihiro Yasunaga, Bing Xiang, Dragomir Radev，ACL，2018.

36. [ Document Embedding Enhanced Event Detection with Hierarchical and Supervised Attention](http://aclweb.org/anthology/P18-2066)Yue Zhao, Xiaolong Jin, Yuanzhuo Wang, Xueqi Cheng，ACL，2018.

37. [Sparse and Constrained Attention for Neural Machine Translation](http://aclweb.org/anthology/P18-2059)Chaitanya Malaviya, Pedro Ferreira, André F. T. Martins，ACL，2018.

38. [Cross-Target Stance Classification with Self-Attention Networks](http://aclweb.org/anthology/P18-2123)Chang Xu, Cecile Paris, Surya Nepal, Ross Sparks，ACL，2018.

39. [A Multi-sentiment-resource Enhanced Attention Network for Sentiment Classification](http://aclweb.org/anthology/P18-2120)Zeyang Lei, Yujiu Yang, Min Yang, Yi Liu，ACL，2018.

40. [Improving Slot Filling in Spoken Language Understanding with Joint Pointer and Attention](http://aclweb.org/anthology/P18-2068)Lin Zhao, Zhe Feng，ACL，2018.

41. [Adversarial Transfer Learning for Chinese Named Entity Recognition with Self-Attention Mechanism](http://aclweb.org/anthology/D18-1018)Nafise Sadat Moosavi, Michael Strube，EMNLP，2018.

42. [Surprisingly Easy Hard-Attention for Sequence to Sequence Learning](http://aclweb.org/anthology/D18-1065)Shiv Shankar, Siddhant Garg, Sunita Sarawagi,EMNLP，2018.

43. [Hybrid Neural Attention for Agreement/Disagreement Inference in Online Debates](http://aclweb.org/anthology/D18-1069)Di Chen, Jiachen Du, Lidong Bing, Ruifeng Xu，EMNLP，2018.

44. [A Hierarchical Neural Attention-based Text Classifier](http://aclweb.org/anthology/D18-1094)Koustuv Sinha, Yue Dong, Jackie Chi Kit Cheung, Derek Ruths，EMNLP，2018.

45. [Supervised Domain Enablement Attention for Personalized Domain Classification](http://aclweb.org/anthology/D18-1106)Joo-Kyung Kim, Young-Bum Kim，EMNLP，2018.

46. [Attention-via-Attention Neural Machine Translation](http://aclweb.org/anthology/D18-1120)Ningyu Zhang, Shumin Deng, Zhanling Sun, Xi Chen, Wei Zhang, Huajun Chen，EMNLP，2018.

47. [Improving Multi-label Emotion Classification via Sentiment Classification with Dual Attention Transfer Network](http://aclweb.org/anthology/D18-1137)Jianfei Yu, Luís Marujo, Jing Jiang, Pradeep Karuturi, William Brendel，EMNLP，2018.

48. [Improving Large-Scale Fact-Checking using Decomposable Attention Models and Lexical Tagging](http://aclweb.org/anthology/D18-1143)Nayeon Lee, Chien-Sheng Wu, Pascale Fung，EMNLP，2018.

49. [Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation](http://aclweb.org/anthology/D18-1156)Xiao Liu, Zhunchen Luo, Heyan Huang，EMNLP，2018.

50. [Collective Event Detection via a Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms](http://aclweb.org/anthology/D18-1158)ubo Chen, Hang Yang, Kang Liu, Jun Zhao, Yantao Jia，EMNLP，2018.

51. [Leveraging Gloss Knowledge in Neural Word Sense Disambiguation by Hierarchical Co-Attention](http://aclweb.org/anthology/D18-1170)Fuli Luo, Tianyu Liu, Zexue He, Qiaolin Xia, Zhifang Sui, Baobao Chang，EMNLP，2018.

52. [Neural Related Work Summarization with a Joint Context-driven Attention Mechanism](http://aclweb.org/anthology/D18-1204)Yongzhen Wang, Xiaozhong Liu, Zheng Gao，EMNLP，2018.

53. [Deriving Machine Attention from Human Rationales](http://aclweb.org/anthology/D18-1216)Yujia Bao, Shiyu Chang, Mo Yu, Regina Barzilay，EMNLP，2018.

54. [Attention-Guided Answer Distillation for Machine Reading Comprehensio](http://aclweb.org/anthology/D18-1232)Minghao Hu, Yuxing Peng, Furu Wei, Zhen Huang, Dongsheng Li, Nan Yang, Ming Zhou，EMNLP，2018.

55. [Multi-Level Structured Self-Attentions for Distantly Supervised Relation Extraction](http://aclweb.org/anthology/D18-1245)Jinhua Du, Jingguang Han, Andy Way, Dadong Wan，EMNLP，2018.


56. [Hierarchical Relation Extraction with Coarse-to-Fine Grained Attention](http://aclweb.org/anthology/D18-1247)Xu Han, Pengfei Yu, Zhiyuan Liu, Maosong Sun, Peng Li，EMNLP，2018.

57. [WECA：A WordNet-Encoded Collocation-Attention Network for Homographic Pun Recognition](http://aclweb.org/anthology/D18-1272)Yufeng Diao, Hongfei Lin, Di Wu, Liang Yang, Kan Xu, Zhihao Yang, Jian Wang, Shaowu Zhang, Bo Xu, Dongyu Zhang，EMNLP，2018.

58. [Multi-Head Attention with Disagreement Regularization](http://aclweb.org/anthology/D18-1317)Jian Li, Zhaopeng Tu, Baosong Yang, Michael R. Lyu, Tong Zhang，EMNLP，2018.

59. [Document-Level Neural Machine Translation with Hierarchical Attention Networks](http://aclweb.org/anthology/D18-1325)Lesly Miculicich Werlen, Dhananjay Ram, Nikolaos Pappas, James Henderson，EMNLP，2018.

60. [Learning When to Concentrate or Divert Attention: Self-Adaptive Attention Temperature for Neural Machine Translation](http://aclweb.org/anthology/D18-1331)Junyang Lin, Xu Sun, Xuancheng Ren, Muyu Li, Qi Su:，EMNLP，2018.

61. [Training Deeper Neural Machine Translation Models with Transparent Attention](http://aclweb.org/anthology/D18-1338)Ankur Bapna, Mia Xu Chen, Orhan Firat, Yuan Cao, Yonghui Wu，EMNLP，2018.

62. [A Genre-Aware Attention Model to Improve the Likability Prediction of Books](http://aclweb.org/anthology/D18-1375)Suraj Maharjan, Manuel Montes-y-Gómez, Fabio A. González, Thamar Solorio，EMNLP，2018.

63. [Multi-grained Attention Network for Aspect-Level Sentiment Classification](http://aclweb.org/anthology/D18-1380)Feifan Fan, Yansong Feng, Dongyan Zhao:，EMNLP，2018.

64. [Attentive Gated Lexicon Reader with Contrastive Contextual Co-Attention for Sentiment Classification](http://aclweb.org/anthology/D18-1381)Yi Tay, Anh Tuan Luu, Siu Cheung Hui, Jian Su，EMNLP，2018.

65. [Contextual Inter-modal Attention for Multi-modal Sentiment Analysis](http://aclweb.org/anthology/D18-1382)Deepanway Ghosal, Md. Shad Akhtar, Dushyant Chauhan, Soujanya Poria, Asif Ekbal, Pushpak Bhattacharyya，EMNLP，2018.

66. [A Visual Attention Grounding Neural Model for Multimodal Machine Translation](http://aclweb.org/anthology/D18-1400)Mingyang Zhou, Runxiang Cheng, Yong Jae Lee, Zhou Yu，EMNLP，2018.

67. [Phrase-level Self-Attention Networks for Universal Sentence Encoding](http://aclweb.org/anthology/D18-1408)Wei Wu, Houfeng Wang, Tianyu Liu, Shuming Ma，EMNLP，2018.

68. [Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Networks](http://aclweb.org/anthology/D18-1424)Yao Zhao, Xiaochuan Ni, Yuanyuan Ding, Qifa Ke，EMNLP，2018.

69. [Abstractive Text-Image Summarization Using Multi-Modal Attentional Hierarchical RNN](http://aclweb.org/anthology/D18-1438)Jingqiang Chen, Hai Zhuge，EMNLP，2018.

70. [Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures](http://aclweb.org/anthology/D18-1458)Gongbo Tang, Mathias Müller, Annette Rios, Rico Sennrich，EMNLP，2018.

71. [Hard Non-Monotonic Attention for Character-Level Transduction](http://aclweb.org/anthology/D18-1473)Shijie Wu, Pamela Shapiro, Ryan Cotterell，EMNLP，2018.

72. [Modeling Localness for Self-Attention Networks](http://aclweb.org/anthology/D18-1475)Baosong Yang, Zhaopeng Tu, Derek F. Wong, Fandong Meng, Lidia S. Chao, Tong Zhang，EMNLP，2018.

73. [Co-Stack Residual Affinity Networks with Multi-level Attention Refinement for Matching Text Sequences](http://aclweb.org/anthology/D18-1479)Yi Tay, Anh Tuan Luu, Siu Cheung Hui，EMNLP，2018.

74. [Learning Universal Sentence Representations with Mean-Max Attention Autoencoder](http://aclweb.org/anthology/D18-1481)Minghua Zhang, Yunfang Wu, Weikang Li, Wei Li，EMNLP，2018.

75. [A Co-attention Neural Network Model for Emotion Cause Analysis with Emotional Context Awareness](http://aclweb.org/anthology/D18-1506)Xiangju Li, Kaisong Song, Shi Feng, Daling Wang, Yifei Zhang，EMNLP，2018.

76. [Interpretable Emoji Prediction via Label-Wise Attention LSTMs](http://aclweb.org/anthology/D18-1508)Francesco Barbieri, Luis Espinosa Anke, José Camacho-Collados, Steven Schockaert, Horacio Saggion，EMNLP，2018.

77. [Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference](http://aclweb.org/anthology/D18-1537)Reza Ghaeini, Xiaoli Z. Fern, Prasad Tadepalli，EMNLP，2018.

78. [Linguistically-Informed Self-Attention for Semantic Role Labeling](http://aclweb.org/anthology/D18-1548)Emma Strubell, Patrick Verga, Daniel Andor, David Weiss, Andrew McCallum，EMNLP，2018.

79. [Discourse-Aware Neural Rewards for Coherent Text Generation](http://aclweb.org/anthology/N18-1016)Antoine Bosselut, Asli Çelikyilmaz, Xiaodong He, Jianfeng Gao, Po-Sen Huang, Yejin Choi，NAACL，2018.

80. [A MIXED HIERARCHICAL ATTENTION BASED ENCODER-DECODER APPROACH FOR STANDARD TABLE SUMMARIZATION ](http://aclweb.org/anthology/N18-2098)Parag Jain, Anirban Laha, Karthik Sankaranarayanan, Preksha Nema, Mitesh M. Khapra, Shreyas Shetty，NAACL，2018.

81. [COMBINING CHARACTER AND WORD INFORMATION IN NEURAL MACHINE TRANSLATION USING A MULTI-LEVEL ATTENTION ](http://aclweb.org/anthology/N18-1116)Huadong Chen, Shujian Huang, David Chiang, Xinyu Dai, Jiajun Chen，NAACL，2018.

82. [Generating Descriptions from Structured Data Using a Bifocal Attention Mechanism and Gated Orthogonalization](http://aclweb.org/anthology/N18-1139)Preksha Nema, Shreyas Shetty, Parag Jain, Anirban Laha, Karthik Sankaranarayanan, Mitesh M. Khapra，NAACL，2018.

83. [Generating Topic-Oriented Summaries Using Neural Attention](http://aclweb.org/anthology/N18-1153)Kundan Krishna, Balaji Vasan Srinivasan，NAACL，2018.

84. [Higher-Order Syntactic Attention Network for Longer Sentence Compression](http://aclweb.org/anthology/N18-1155)Hidetaka Kamigaito, Katsuhiko Hayashi, Tsutomu Hirao, Masaaki Nagata，NAACL，2018.

85. [How Time Matters: Learning Time-Decay Attention for Contextual Spoken Language Understanding in Dialogues](http://aclweb.org/anthology/N18-1194)Shang-Yu Su, Pei-Chieh Yuan, Yun-Nung Chen，NAACL，2018.

86. [Knowledge-Enriched Two-Layered Attention Network for Sentiment Analysis](http://aclweb.org/anthology/N18-2041)Abhishek Kumar, Daisuke Kawahara, Sadao Kurohashi，NAACL，2018.

87. [Self-Attention with Relative Position Representations](http://aclweb.org/anthology/N18-2074)Peter Shaw, Jakob Uszkoreit, Ashish Vaswani，NAACL，2018.

88. [Target Foresight Based Attention for Neural Machine Translation](http://aclweb.org/anthology/N18-1125)Xintong Li, Lemao Liu, Zhaopeng Tu, Shuming Shi, Max Meng，NAACL，2018.

89. [Watch, Listen, and Describe: Globally and Locally Aligned Cross-Modal Attentions for Video Captioning](http://aclweb.org/anthology/N18-2125)Xin Wang, Yuan-Fang Wang, William Yang Wang，NAACL，2018.

90. [Query and Output: Generating Words by Querying Distributed Word Representations for Paraphrase Generation](http://aclweb.org/anthology/N18-1018)Shuming Ma, Xu Sun, Wei Li, Sujian Li, Wenjie Li, Xuancheng Ren，NAACL，2018.

91. [Medical Concept Embedding with Time-Aware Attention](https://www.ijcai.org/proceedings/2018/0554.pdf)Xiangrui Cai, Jinyang Gao, Kee Yuan Ngiam, Beng Chin Ooi, Ying Zhang, Xiaojie Yuan，IJCAI，2018.

92. [Attention-Fused Deep Matching Network for Natural Language Inference](https://www.ijcai.org/proceedings/2018/0561.pdf)Chaoqun Duan, Lei Cui, Xinchi Chen, Furu Wei, Conghui Zhu, Tiejun Zhao，IJCAI，2018.

93. [Multi-modal Sentence Summarization with Modality Attention and Image Filtering](https://www.ijcai.org/proceedings/2018/0577.pdf)Haoran Li, Junnan Zhu, Tianshang Liu, Jiajun Zhang, Chengqing Zong，IJCAI，2018.

94. [Code Completion with Neural Attention and Pointer Networks](https://www.ijcai.org/proceedings/2018/0578.pdf)Jian Li, Yue Wang, Michael R. Lyu, Irwin King，IJCAI，2018.

95. [Aspect Term Extraction with History Attention and Selective Transformation](https://www.ijcai.org/proceedings/2018/0583.pdf)Xin Li, Lidong Bing, Piji Li, Wai Lam, Zhimou Yang，IJCAI，2018.

96. [Feature Enhancement in Attention for Visual Question Answering](https://www.ijcai.org/proceedings/2018/0586.pdf)Yuetan Lin, Zhangyang Pang, Donghui Wang, Yueting Zhuang，IJCAI，2018.

97. [Beyond Polarity: Interpretable Financial Sentiment Analysis with Hierarchical Query-driven Attention](https://www.ijcai.org/proceedings/2018/0590.pdf)Ling Luo, Xiang Ao, Feiyang Pan, Jin Wang, Tong Zhao, Ningzi Yu, Qing He，IJCAI，2018.

98. [Translating Embeddings for Knowledge Graph Completion with Relation Attention Mechanism](https://www.ijcai.org/proceedings/2018/0596.pdf)Wei Qian, Cong Fu, Yu Zhu, Deng Cai, Xiaofei He，IJCAI，2018.

99. [Reinforced Self-Attention Network: a Hybrid of Hard and Soft Attention for Sequence Modeling](https://www.ijcai.org/proceedings/2018/0604.pdf)Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Sen Wang, Chengqi Zhang，IJCAI，2018.

100. [Listen, Think and Listen Again: Capturing Top-down Auditory Attention for Speaker-independent Speech Separation](https://www.ijcai.org/proceedings/2018/0605.pdf)Jing Shi, Jiaming Xu, Guangcan Liu, Bo Xu，IJCAI，2018.

101. [Multiway Attention Networks for Modeling Sentence Pairs](https://www.ijcai.org/proceedings/2018/0613.pdf)Chuanqi Tan, Furu Wei, Wenhui Wang, Weifeng Lv, Ming Zhou，IJCAI，2018.

102. [Get The Point of My Utterance! Learning Towards Effective Responses with Multi-Head Attention Mechanism](https://www.ijcai.org/proceedings/2018/0614.pdf)Chongyang Tao, Shen Gao, Mingyue Shang, Wei Wu, Dongyan Zhao, Rui Yan，IJCAI，2018.

103. [Hermitian Co-Attention Networks for Text Matching in Asymmetrical Domains](https://www.ijcai.org/proceedings/2018/0615.pdf)Yi Tay, Anh Tuan Luu, Siu Cheung Hui，IJCAI，2018.

104. [Aspect Sentiment Classification with both Word-level and Clause-level Attention Networks](https://www.ijcai.org/proceedings/2018/0617.pdf)Jingjing Wang, Jie Li, Shoushan Li, Yangyang Kang, Min Zhang, Luo Si, Guodong Zhou，IJCAI，2018.

105. [Densely Connected CNN with Multi-scale Feature Attention for Text Classification](https://www.ijcai.org/proceedings/2018/0621.pdf)Shiyao Wang, Minlie Huang, Zhidong Deng，IJCAI，2018.

106. [Same Representation, Different Attentions: Shareable Sentence Representation Learning from Multiple Tasks](https://www.ijcai.org/proceedings/2018/0642.pdf)Renjie Zheng, Junkun Chen, Xipeng Qiu，IJCAI，2018.

107. [Commonsense Knowledge Aware Conversation Generation with Graph Attention](https://www.ijcai.org/proceedings/2018/0643.pdf)Hao Zhou, Tom Young, Minlie Huang, Haizhou Zhao, Jingfang Xu, Xiaoyan Zhu，IJCAI，2018.


# [2019] 

1. [To Find Where You Talk: Temporal Sentence Localization in Video with Attention Based Location Regression](https://arxiv.org/abs/1804.07014) Yitian Yuan, Tao Mei, Wenwu Zhu, AAAI, 2019.

2. [An Affect-Rich Neural Conversational Model with Biased Attention and Weighted Cross-Entropy Loss](https://arxiv.org/abs/1811.07078) Peixiang Zhong, Di Wang, Chunyan Miao, AAAI, 2019.

3. [Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding](https://arxiv.org/abs/1811.01399) Peifeng Wang, Jialong Han, Chenliang Li, Rong Pan, AAAI, 2019.

4. [Cross-relation Cross-bag Attention for Distantly-supervised Relation Extraction](https://arxiv.org/abs/1812.10604) Yujin Yuan, Liyuan Liu, Siliang Tang, Zhongfei Zhang, Yueting Zhuang, Shiliang Pu, Fei Wu, Xiang Ren , AAAI, 2019.

5. [Deep Metric Learning by Online Soft Mining and Class-Aware Attention](https://arxiv.org/abs/1811.01459) Xinshao Wang, Yang Hua, Elyor Kodirov, Guosheng Hu, Neil M. Robertson, AAAI, 2019.

6. [Multi-Task Learning with Multi-View Attention for Answer Selection and Knowledge Base Question Answering](https://arxiv.org/abs/1812.02354) Yang Deng, Yuexiang Xie, Yaliang Li, Min Yang, Nan Du, Wei Fan, Kai Lei, Ying Shen , AAAI, 2019.

7. [Character-Level Language Modeling with Deeper Self-Attention](https://arxiv.org/abs/1808.04444) Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, Llion Jones , AAAI, 2019.

8. [Convolutional Spatial Attention Model for Reading Comprehension with Multiple-Choice Questions](https://arxiv.org/abs/1811.08610) Zhipeng Chen, Yiming Cui, Wentao Ma, Shijin Wang, Guoping Hu, AAAI, 2019.


